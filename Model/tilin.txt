import SwiftUI
import CoreML
import Vision

import UIKit

struct ModeloView: View {
    @State private var image: UIImage? = nil
    @State private var showImagePicker: Bool = false
    @State private var classificationLabel: String = ""
    @StateObject private var cameraViewModel = CameraViewController()

    var body: some View {
        ZStack{
            Color.black
                .ignoresSafeArea(.all)
            VStack {
                Text(classificationLabel)
                    .padding()
                    .foregroundColor(.white)

                if let image = image {
                    Image(uiImage: image)
                        .resizable()
                        .scaledToFit()
                }

                Button("Tomate la foto") {
                    self.showImagePicker = true
                }
            }
            .sheet(isPresented: $showImagePicker) {
                ImagePicker(image: self.$image, classificationLabel: self.$classificationLabel, cameraViewModel: cameraViewModel)
            }
        }
        
    }
}

struct ImagePicker: UIViewControllerRepresentable {
    @Environment(\.presentationMode) var presentationMode
    @Binding var image: UIImage?
    @Binding var classificationLabel: String
    var cameraViewModel: CameraViewController

    func makeUIViewController(context: Context) -> UIImagePickerController {
        let picker = UIImagePickerController()
        picker.delegate = context.coordinator
        picker.sourceType = .camera
        return picker
    }

    func updateUIViewController(_ uiViewController: UIImagePickerController, context: Context) {}

    func makeCoordinator() -> Coordinator {
        Coordinator(self, cameraViewModel: cameraViewModel)
    }

    class Coordinator: NSObject, UIImagePickerControllerDelegate, UINavigationControllerDelegate {
        var parent: ImagePicker
        var cameraViewModel: CameraViewController

        init(_ parent: ImagePicker, cameraViewModel: CameraViewController) {
            self.parent = parent
            self.cameraViewModel = cameraViewModel
        }

        func imagePickerController(_ picker: UIImagePickerController, didFinishPickingMediaWithInfo info: [UIImagePickerController.InfoKey: Any]) {
            if let uiImage = info[.originalImage] as? UIImage {
                parent.image = uiImage
                classifyImage(image: uiImage)
            }

            parent.presentationMode.wrappedValue.dismiss()
        }

        func classifyImage(image: UIImage) {
            guard let buffer = image.toCVPixelBuffer() else {
                print("Failed to convert UIImage to CVPixelBuffer")
                return
            };do {
                let model = try EmotionClassifier(configuration: MLModelConfiguration())
                let prediction = try model.prediction(conv2d_input: buffer)
                DispatchQueue.main.async {
                    self.parent.classificationLabel = prediction.classLabel
                }

                // Aquí puedes llamar a la función addPicture del ViewModel
                let dateFormatter = DateFormatter()
                dateFormatter.dateFormat = "yyyy-MM-dd"
                let currentDateStr = dateFormatter.string(from: Date())
                
                let imageData = image.jpegData(compressionQuality: 0.5)
                let base64Image = imageData?.base64EncodedString()
                
                print(base64Image ?? "")
                print(1)
                print(currentDateStr)

                

                if let base64Image = base64Image, let imageData = Data(base64Encoded: base64Image) {
                    let image = UIImage(data: imageData)
                    cameraViewModel.addPicture(image: image ?? UIImage(), user: 1, date: currentDateStr)
                } else {
                    print("Error: no se pudo convertir la imagen base64 a UIImage.")
                }
            } catch {
                print("Error while making a prediction: \(error)")
            }
        }
    }
}
            // Aquí agregarías tu lógica de clasificación
    
    
    
    
extension UIImage {
    func toCVPixelBuffer() -> CVPixelBuffer? {
        let width = 48
        let height = 48
        let size = CGSize(width: width, height: height)
        
        UIGraphicsBeginImageContextWithOptions(size, true, 1.0)
        self.draw(in: CGRect(x: 0, y: 0, width: size.width, height: size.height))
        let resizedImage = UIGraphicsGetImageFromCurrentImageContext()!
        UIGraphicsEndImageContext()
        
        let attrs = [kCVPixelBufferCGImageCompatibilityKey: kCFBooleanTrue,
             kCVPixelBufferCGBitmapContextCompatibilityKey: kCFBooleanTrue] as CFDictionary
        var pixelBuffer: CVPixelBuffer?
        
        let status = CVPixelBufferCreate(kCFAllocatorDefault,
                                         width,
                                         height,
                                         kCVPixelFormatType_OneComponent8,
                                         attrs,
                                         &pixelBuffer)
        
        if status != kCVReturnSuccess {
            return nil
        }
        
        CVPixelBufferLockBaseAddress(pixelBuffer!, CVPixelBufferLockFlags(rawValue: 0))
        let pixelData = CVPixelBufferGetBaseAddress(pixelBuffer!)
        
        let grayscaleColorSpace = CGColorSpaceCreateDeviceGray()
        guard let context = CGContext(data: pixelData,
                                      width: width,
                                      height: height,
                                      bitsPerComponent: 8,
                                      bytesPerRow: CVPixelBufferGetBytesPerRow(pixelBuffer!),
                                      space: grayscaleColorSpace,
                                      bitmapInfo: CGImageAlphaInfo.none.rawValue) else {
            return nil
        }
        
        UIGraphicsPushContext(context)
        guard let cgImage = resizedImage.cgImage else { return nil }
        context.draw(cgImage, in: CGRect(x: 0, y: 0, width: width, height: height))
        UIGraphicsPopContext()
        CVPixelBufferUnlockBaseAddress(pixelBuffer!, CVPixelBufferLockFlags(rawValue: 0))
        
        return pixelBuffer
    }
}

struct ModeloView_Previews: PreviewProvider {
    static var previews: some View {
        ModeloView()
    }
}


//
//  CameraViewController.swift
//  ModeloViewPruebas
//
//  Created by Jesús Daniel Martínez García on 10/11/23.
//

import AVFoundation
import UIKit
import Alamofire

class CameraViewController: UIViewController, UIImagePickerControllerDelegate, UINavigationControllerDelegate, ObservableObject {
    
    @Published var url : String = ""
    
    override func viewDidLoad() {
        super.viewDidLoad()
        checkCameraAccess()
    }
    
    func checkCameraAccess() {
        switch AVCaptureDevice.authorizationStatus(for: .video) {
        case .authorized: // El usuario ya autorizó el acceso a la cámara
            break
        case .notDetermined: // No se ha solicitado el permiso aún
            AVCaptureDevice.requestAccess(for: .video) { granted in
                if !granted {
                    // El usuario no otorgó el permiso
                }
            }
        default: // El permiso fue negado
            break
        }
    }
    
    @IBAction func takePhotoButtonTapped(_ sender: Any) {
        let imagePickerController = UIImagePickerController()
        imagePickerController.sourceType = .camera
        imagePickerController.delegate = self
        present(imagePickerController, animated: true, completion: nil)
    }
    @Published var networkError: String?
    
    
    func addPicture(image: UIImage, user: Int, date: String) {
    // Preparamos el endpoint y los parámetros
    let requestURL = URL(string: "https://philbackend.onrender.com/api/auth/AddPicture") // Cambia esto a tu URL real
    var request = URLRequest(url: requestURL!)
    request.httpMethod = "POST"
    request.setValue("application/json", forHTTPHeaderField: "Content-Type")
    
    // Reducir el tamaño de la imagen
    let resizedImage = resizeImage(image: image, newWidth: 300)
    
    // Convertir la imagen a Data y luego a una cadena Base64
    guard let imageData = resizedImage.jpegData(compressionQuality: 0.5) else {
        print("No se pudo obtener los datos de la imagen.")
        return
    }
    let base64Image = imageData.base64EncodedString()
    
    let parameters: [String: Any] = [
        "url": base64Image,
        "user": user,
        "date": date // Ya no necesitas formatear 'date' ya que ya es un String.
    ]
    
    do {
        request.httpBody = try JSONSerialization.data(withJSONObject: parameters, options: .prettyPrinted)
    } catch let error {
        print("Error al añadir la imagen: \(error.localizedDescription)")
        self.networkError = error.localizedDescription
        return
    }
    
    // Realizamos la solicitud POST
    let task = URLSession.shared.dataTask(with: request) { (data, response, error) in
        if let error = error {
            print("Error al añadir la imagen: \(error.localizedDescription)")
            self.networkError = error.localizedDescription
        } else if let httpResponse = response as? HTTPURLResponse, 200..<300 ~= httpResponse.statusCode {
            print("Imagen añadida con éxito.")
        } else {
            print("Error al añadir la imagen: respuesta no válida.")
            self.networkError = "Respuesta no válida."
            if let data = data, let body = String(data: data, encoding: .utf8) {
                print("Cuerpo de la respuesta: \(body)")
            }
        }
    }
    
    task.resume()
}

func resizeImage(image: UIImage, newWidth: CGFloat) -> UIImage {
    let scale = newWidth / image.size.width
    let newHeight = image.size.height * scale
    UIGraphicsBeginImageContext(CGSize(width: newWidth, height: newHeight))
    image.draw(in: CGRect(x: 0, y: 0, width: newWidth, height: newHeight))
    let newImage = UIGraphicsGetImageFromCurrentImageContext()
    UIGraphicsEndImageContext()
    
    return newImage!
}
}
    

